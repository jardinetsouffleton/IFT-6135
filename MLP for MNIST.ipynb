{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, programming part\n",
    "#### IFT 6135\n",
    "#### February 17, 2019 23:59\n",
    "#### Authors: Leo Boisvert(20032421), Andrew Williams(20125276)\n",
    "#### Link to github repository: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils.mnist_reader as mnist_reader\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "filepath = r'C:\\Users\\User\\Desktop\\UDEM\\IFT6135H19_assignment-master\\mnist.pkl.gz'\n",
    "\n",
    "f = gzip.open(filepath, 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_set\n",
    "X_test, y_test = test_set\n",
    "X_valid, Y_valid = valid_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, d, d_h,d_h2, m, hidden_dims=(512,512),n_hidden=2,mode='train', datapath=None,model_path=None):\n",
    "        self.m = m\n",
    "        self.d_h = d_h\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_hidden = n_hidden\n",
    "        self.mode = mode\n",
    "        self.datapath = datapath\n",
    "        self.model_path = model_path\n",
    "        self.binary_y = 0\n",
    "        \n",
    "        #parameters\n",
    "        self.W1 = np.zeros((d_h, d))\n",
    "        self.W2 = np.zeros((d_h2, d_h))\n",
    "        self.W3 = np.zeros((m, d_h2))\n",
    "        \n",
    "        self.b1 = np.zeros((d_h, 1))\n",
    "        self.b2 = np.zeros((d_h2, 1))\n",
    "        self.b3 = np.zeros((m, 1))\n",
    "        \n",
    "        #activations\n",
    "        self.os = 0   \n",
    "        self.oa = 0   \n",
    "        \n",
    "        self.hs1 = 0   \n",
    "        self.ha1 = 0   \n",
    "        \n",
    "        self.hs2 = 0 \n",
    "        self.ha2 = 0\n",
    "        \n",
    "        #gradients\n",
    "        self.dW1 = np.zeros((d_h, d))\n",
    "        self.dW2 = np.zeros((d_h2, d_h))\n",
    "        self.dW3 = np.zeros((m, d_h2))\n",
    "        \n",
    "        self.db1 = np.zeros((d_h, 1))\n",
    "        self.db2 = np.zeros((d_h2, 1))  \n",
    "        self.db3 = np.zeros((m, 1))\n",
    "        \n",
    "        #losses and missclassification rates\n",
    "        self.loss_train =[]\n",
    "        self.missclass_train = []\n",
    "        \n",
    "        self.loss_valid = []\n",
    "        self.missclass_valid = []\n",
    "        \n",
    "        self.loss_test = []\n",
    "        self.missclass_test = []\n",
    "        \n",
    "        \n",
    "    def initialize_weights(self, n_hidden=2, dims=(784,10), initialization = None):\n",
    "        if initialization == None:\n",
    "            pass\n",
    "            \n",
    "        if initialization == 'normal':\n",
    "            self.W1 = np.random.normal(0,1 , (self.hidden_dims[0],dims[0]))\n",
    "            self.W2 = np.random.normal(0,1 , (self.hidden_dims[1],self.hidden_dims[0]))\n",
    "            self.W3 = np.random.normal(0,1 , (dims[1],self.hidden_dims[1]))\n",
    "            \n",
    "        if initialization == 'glorot':\n",
    "            self.W1 = np.random.uniform((-1*np.sqrt(6/(784+512))),(np.sqrt(6/(784+512))), \\\n",
    "                                        (self.hidden_dims[0],dims[0]))\n",
    "            self.W2 = np.random.uniform((-1*np.sqrt(6/(512+512))),(np.sqrt(6/(512+512))), \\\n",
    "                                        (self.hidden_dims[1],self.hidden_dims[0]))\n",
    "            self.W3 = np.random.uniform((-1*np.sqrt(6/(10+512))),(np.sqrt(6/(10+512))), \\\n",
    "                                        (dims[1],self.hidden_dims[1]))\n",
    "    \n",
    "    \n",
    "    def forward(self, X):        \n",
    "        #first layer prop\n",
    "        self.ha = np.dot(self.W1, X.T) + self.b1\n",
    "        self.hs = self.activation(self.ha)\n",
    "        \n",
    "        #second layer prop\n",
    "        self.ha2 = np.dot(self.W2, self.hs) + self.b2\n",
    "        self.hs2 = self.activation(self.ha2)\n",
    "        \n",
    "        #third layer prop\n",
    "        self.oa = np.dot(self.W3, self.hs2) + self.b3\n",
    "        self.os = self.softmax()\n",
    "     \n",
    "    \n",
    "    def activation(self, fct):\n",
    "        activation = [[max(0, i) for i in row] for row in fct]\n",
    "        return np.array(activation)\n",
    "    \n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        t = self.getbinary(Y)\n",
    "        self.forward(X)\n",
    "        K = self.os.shape[1]\n",
    "        if K == t.shape[1]:\n",
    "            return -1 * self.m * np.mean(t * np.log(self.os))\n",
    "        else :\n",
    "          print(\"The subset of example is not consistent between X and Y\")             \n",
    "    \n",
    "    \n",
    "    def softmax(self):\n",
    "        shift = np.max(self.oa, axis = 0)\n",
    "        shifted_output = self.oa - shift\n",
    "        normal = np.sum(np.exp(shifted_output), axis = 0)\n",
    "        return np.exp(shifted_output) / normal\n",
    "    \n",
    "    \n",
    "    def backward(self, X, Y, lamb = np.zeros((3, 2))):\n",
    "        t = self.getbinary(Y)\n",
    "        K = X.shape[0]\n",
    "        \n",
    "        if K == t.shape[1]:\n",
    "            #backprop through activation\n",
    "            doa = self.os - t     # (m, K)\n",
    "            self.dW3 = 1 / K * np.dot(doa, self.hs2.T)   # (m, d_h)\n",
    "            self.db3 = np.mean(doa, axis = 1, keepdims = True)  # (m, 1)\n",
    "            \n",
    "            #backprop through second hidden layer\n",
    "            dha2 = np.dot(self.W3.T, doa) * self.positive(self.ha2) \n",
    "            self.dW2 = 1 / K * np.dot(dha2, self.hs.T)\n",
    "            self.db2 = np.mean(dha2, axis = 1, keepdims = True)\n",
    "            \n",
    "            #backprop through first hidden layer\n",
    "            dha = np.dot(self.W2.T, dha2) * self.positive(self.ha)  # (d_h, K)\n",
    "            self.dW1 = 1 / K * np.dot(dha, X)\n",
    "            self.db1 = np.mean(dha, axis = 1, keepdims = True)  #(d_h, 1)\n",
    "            \n",
    "            #regularization\n",
    "            self.dW3 += lamb[0][0]*np.sign(self.W3) + 2 * lamb[0][1] * self.W3\n",
    "            self.dW2 += lamb[1][0]*np.sign(self.W2) + 2 * lamb[1][1] * self.W2\n",
    "            self.dW1 += lamb[2][0]*np.sign(self.W1) + 2 * lamb[2][1] * self.W1\n",
    "              \n",
    "        else:\n",
    "            print(\"The subset of example is not consistent between X and Y\")\n",
    "    \n",
    "    \n",
    "    def update(self, eta):\n",
    "        self.W3 -= eta * self.dW3\n",
    "        self.b3 -= eta * self.db3\n",
    "        \n",
    "        self.W2 -= eta * self.dW2\n",
    "        self.b2 -= eta * self.db2\n",
    "        \n",
    "        self.W1 -= eta * self.dW1\n",
    "        self.b1 -= eta * self.db1\n",
    "                \n",
    "                \n",
    "    def train(self, X, Y, K, eta=0.07, epsilon=1e-3, max_ite=50000, X_valid=np.array([]), Y_valid=np.array([]), \\\n",
    "              n_rep_early_stopping=2, print_loss = False, lamb = np.zeros((3, 2)), X_test=[], \\\n",
    "              Y_test=[], print_graphs= False, initialization = 'glorot'):\n",
    "        \n",
    "        #bool that decides if early stopping is used\n",
    "        early_stopping = (X_valid.size != 0)\n",
    "        if early_stopping:\n",
    "            valid_error = self.compute_missclass(X_valid, Y_valid)\n",
    "        \n",
    "        #weight and iteration initialization\n",
    "        self.initialize_weights(initialization = initialization)\n",
    "        cond = True\n",
    "        ite = 0\n",
    "        ite_overfit = 0\n",
    "        \n",
    "        while cond:   \n",
    "            ## generate subset of training data\n",
    "            X_minibatch = X[[(i % X.shape[0]) for i in range(ite, ite + K)],:]\n",
    "            Y_minibatch = Y[[(i % X.shape[0]) for i in range(ite, ite + K)]]\n",
    "            \n",
    "            #training iteration\n",
    "            self.forward(X_minibatch)\n",
    "            self.backward(X_minibatch, Y_minibatch, lamb)\n",
    "            self.update(eta = eta)\n",
    "            \n",
    "            #if start of training or epoch finishes\n",
    "            if (ite)%5000 ==0:\n",
    "                if print_loss:\n",
    "                    print('Missclassification rate after epoch ', ite/5000, ' : ' , self.compute_missclass(X, Y))\n",
    "                    print('Average loss after epoch ', ite/5000, ' : ', self.loss(X,Y))\n",
    "                    \n",
    "                if print_graphs:\n",
    "                    self.loss_train.append(self.loss(X, Y))\n",
    "                    self.missclass_train.append(self.compute_missclass(X, Y))\n",
    "                    \n",
    "                    self.loss_valid.append(self.loss(X_valid, Y_valid))\n",
    "                    self.missclass_valid.append(self.compute_missclass(X_valid, Y_valid)) \n",
    "                    \n",
    "                    self.loss_test.append(self.loss(X_test, Y_test))\n",
    "                    self.missclass_test.append(self.compute_missclass(X_test, Y_test)) \n",
    "                \n",
    "                #smaller stepsize\n",
    "                if ite==35000:\n",
    "                    eta = 0.01\n",
    "                \n",
    "                #even smaller stepsize\n",
    "                if ite==40000:\n",
    "                    eta = 0.003\n",
    "                \n",
    "                if early_stopping:\n",
    "                    valid_error_after = self.compute_missclass(X_valid, Y_valid)\n",
    "                    #print(valid_error_after)\n",
    "                    if valid_error_after > valid_error:\n",
    "                        ite_overfit += 1\n",
    "                    else:\n",
    "                        ite_overfit = 0\n",
    "                    valid_error = valid_error_after\n",
    "                \n",
    "                if ite_overfit > n_rep_early_stopping:\n",
    "                    print(\"Exit to avoid overfit\")\n",
    "                    break \n",
    "            \n",
    "            ite += 1\n",
    "            \n",
    "            #training either converges or we run out of iterations\n",
    "            cond = self.norm_grad() > epsilon and ite < max_ite\n",
    "            \n",
    "        print(\"Missclassification ratio on training set\"+ str(self.compute_missclass(X, Y)*100)+ \" %\")\n",
    "        \n",
    "        if early_stopping:\n",
    "            print(\"Missclassification ratio on validation set\"+ str(self.compute_missclass(X_valid, Y_valid)*100)+ \" %\")\n",
    "    \n",
    "    \n",
    "    def test(self, X, Y):\n",
    "        print('test loss: ', self.loss(X,Y))\n",
    "        print('test missclassification rate: ', self.compute_missclass(X,Y))\n",
    "        \n",
    "##############################################################################\n",
    "######################### additional functionalities #########################\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        return np.argmax(self.os, axis = 0)\n",
    "    \n",
    "    \n",
    "    def positive(self, fct):\n",
    "        positive = [[ int(i > 0) for i in row] for row in fct]\n",
    "        return np.array(positive)\n",
    "    \n",
    "    \n",
    "    #convert Y (K X 1) to a target matrix (m X K)\n",
    "    def getbinary(self, Y):\n",
    "        try: \n",
    "            return np.array([[int(Y[j] == i) for j in range(Y.shape[0])] for i in range(self.m)])\n",
    "        except IndexError:\n",
    "            return np.transpose(np.array([[int(Y == i)] for i in range(self.m)]))\n",
    "    \n",
    "    \n",
    "    #computes missclassification ratio      \n",
    "    def compute_missclass(self, X, Y):\n",
    "        t = self.getbinary(Y)\n",
    "        self.forward(X)\n",
    "        K = self.os.shape[1]\n",
    "        if K == t.shape[1]:\n",
    "            predictions = np.argmax(self.os, axis = 0)\n",
    "            return np.mean(predictions!=Y)\n",
    "        else :\n",
    "          print(\"The subset of example is not consistent between X and Y\") \n",
    "    \n",
    "    \n",
    "    #returns the norm of the gradient      \n",
    "    def norm_grad(self):\n",
    "        return np.sqrt(np.sum(self.dW3**2)+np.sum(self.db3**2)+np.sum(self.dW2**2)+np.sum(self.dW1**2)+\\\n",
    "                           np.sum(self.db2**2)+np.sum(self.db1**2))\n",
    "    \n",
    "    \n",
    "    #returns the approximative gradient using mini batches of size K\n",
    "    def finite_differences(self, X, Y, epsilon, print_grads=False):\n",
    "        differences_plus = np.empty(10)\n",
    "        differences_minus = np.empty(10)\n",
    "        finite_grad = np.empty(10)\n",
    "        \n",
    "        for i in range(10):\n",
    "            self.W2[0][i] += epsilon\n",
    "            differences_plus[i] = self.loss(X.reshape(-1,1).T,Y.reshape(-1,1).T)\n",
    "            self.W2[0][i] -= 2*epsilon\n",
    "            differences_minus[i] = self.loss(X.reshape(-1,1).T,Y.reshape(-1,1).T)\n",
    "            self.W2[0][i] += epsilon\n",
    "            finite_grad[i] = ((differences_plus[i]) - (differences_minus[i]))/(2*epsilon)\n",
    "        return finite_grad\n",
    "        \n",
    "        if print_grads:\n",
    "                print('dW3 = ', self.dW3)\n",
    "        else: return (self.dW3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "    Aside from the weight initializations, the models were trained using the optimal hyperparameters foudn in the subsequent question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missclassification rate after epoch  0.0  :  0.88644\n",
      "Average loss after epoch  0.0  :  2.3024704054960825\n",
      "Missclassification rate after epoch  1.0  :  0.88644\n",
      "Average loss after epoch  1.0  :  2.3109781547380757\n",
      "Missclassification rate after epoch  2.0  :  0.90136\n",
      "Average loss after epoch  2.0  :  2.310081140655338\n",
      "Missclassification rate after epoch  3.0  :  0.88644\n",
      "Average loss after epoch  3.0  :  2.315347052145713\n",
      "Missclassification rate after epoch  4.0  :  0.90988\n",
      "Average loss after epoch  4.0  :  2.3110326359809257\n",
      "Missclassification rate after epoch  5.0  :  0.88644\n",
      "Average loss after epoch  5.0  :  2.3057777599251135\n",
      "Missclassification rate after epoch  6.0  :  0.8965\n",
      "Average loss after epoch  6.0  :  2.3103414903362154\n",
      "Missclassification rate after epoch  7.0  :  0.8965\n",
      "Average loss after epoch  7.0  :  2.3095665972231085\n",
      "Missclassification rate after epoch  8.0  :  0.88644\n",
      "Average loss after epoch  8.0  :  2.30174839000201\n",
      "Missclassification rate after epoch  9.0  :  0.88644\n",
      "Average loss after epoch  9.0  :  2.3013912599475463\n",
      "Missclassification ratio on training set88.644 %\n",
      "Missclassification ratio on validation set89.36 %\n",
      "Missclassification rate after epoch  0.0  :  0.79268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:100: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss after epoch  0.0  :  nan\n",
      "Missclassification rate after epoch  1.0  :  0.88644\n",
      "Average loss after epoch  1.0  :  2.3109781547333803\n",
      "Missclassification rate after epoch  2.0  :  0.90136\n",
      "Average loss after epoch  2.0  :  2.310081140655338\n",
      "Missclassification rate after epoch  3.0  :  0.88644\n",
      "Average loss after epoch  3.0  :  2.315347052145713\n",
      "Missclassification rate after epoch  4.0  :  0.90988\n",
      "Average loss after epoch  4.0  :  2.3110326359809257\n",
      "Missclassification rate after epoch  5.0  :  0.88644\n",
      "Average loss after epoch  5.0  :  2.3057777599251135\n",
      "Missclassification rate after epoch  6.0  :  0.8965\n",
      "Average loss after epoch  6.0  :  2.3103414903362154\n",
      "Missclassification rate after epoch  7.0  :  0.8965\n",
      "Average loss after epoch  7.0  :  2.3095665972231085\n",
      "Missclassification rate after epoch  8.0  :  0.88644\n",
      "Average loss after epoch  8.0  :  2.30174839000201\n",
      "Missclassification rate after epoch  9.0  :  0.88644\n",
      "Average loss after epoch  9.0  :  2.3013912599475463\n",
      "Missclassification ratio on training set88.644 %\n",
      "Missclassification ratio on validation set89.36 %\n",
      "Missclassification rate after epoch  0.0  :  0.86132\n",
      "Average loss after epoch  0.0  :  2.2623199891972536\n",
      "Missclassification rate after epoch  1.0  :  0.1058\n",
      "Average loss after epoch  1.0  :  0.40136604133436116\n",
      "Missclassification rate after epoch  2.0  :  0.07016\n",
      "Average loss after epoch  2.0  :  0.2541332180528993\n",
      "Missclassification rate after epoch  3.0  :  0.05888\n",
      "Average loss after epoch  3.0  :  0.21129580698761238\n",
      "Missclassification rate after epoch  4.0  :  0.06042\n",
      "Average loss after epoch  4.0  :  0.2242314447392826\n",
      "Missclassification rate after epoch  5.0  :  0.04562\n",
      "Average loss after epoch  5.0  :  0.15642988480691805\n",
      "Missclassification rate after epoch  6.0  :  0.04606\n",
      "Average loss after epoch  6.0  :  0.16691329396366023\n",
      "Missclassification rate after epoch  7.0  :  0.03164\n",
      "Average loss after epoch  7.0  :  0.10858676083417973\n",
      "Missclassification rate after epoch  8.0  :  0.02746\n",
      "Average loss after epoch  8.0  :  0.09047172432996828\n",
      "Missclassification rate after epoch  9.0  :  0.02746\n",
      "Average loss after epoch  9.0  :  0.08842244714621039\n",
      "Missclassification ratio on training set2.348 %\n",
      "Missclassification ratio on validation set2.78 %\n"
     ]
    }
   ],
   "source": [
    "#Zero weights \n",
    "NN_Mnist_zero = NN(784,512,512,10)\n",
    "NN_Mnist_zero.train(X_train, y_train, 10, eta=0.05, max_ite = 50000 , epsilon=1e-6, print_loss=True,\n",
    "                    X_valid = X_valid, Y_valid= Y_valid, X_test = X_test, Y_test = y_test, \n",
    "                    lamb = ((0,0.00005),(0,0.00005),(0,0.00005)), print_graphs=True, initialization = None)\n",
    "\n",
    "#normal weights\n",
    "NN_Mnist_normal = NN(784,512,512,10)\n",
    "NN_Mnist_normal.train(X_train, y_train, 10, eta=0.05, max_ite = 50000 , epsilon=1e-6, print_loss=True,\n",
    "                      X_valid = X_valid, Y_valid= Y_valid, X_test = X_test, Y_test = y_test, \n",
    "                      lamb = ((0,0.00005),(0,0.00005),(0,0.00005)), print_graphs=True, initialization = 'normal')\n",
    "\n",
    "#glorot weights\n",
    "NN_Mnist_glorot = NN(784,512,512,10)\n",
    "NN_Mnist_glorot.train(X_train, y_train, 10, eta=0.05, max_ite = 50000 , epsilon=1e-6, print_loss=True,\n",
    "                      X_valid = X_valid, Y_valid= Y_valid, X_test = X_test, Y_test = y_test, \n",
    "                      lamb = ((0,0.00005),(0,0.00005),(0,0.00005)), print_graphs=True, initialization = 'glorot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3024704054960825,\n",
       " 2.3109781547380757,\n",
       " 2.310081140655338,\n",
       " 2.315347052145713,\n",
       " 2.3110326359809257,\n",
       " 2.3057777599251135,\n",
       " 2.3103414903362154,\n",
       " 2.3095665972231085,\n",
       " 2.30174839000201,\n",
       " 2.3013912599475463]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average losses at the end of each epoch\n",
    "#and graphs\n",
    "NN_Mnist_zero.loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss by initialization and Epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>2.30247</td>\n",
       "      <td>2.310978</td>\n",
       "      <td>2.310081</td>\n",
       "      <td>2.315347</td>\n",
       "      <td>2.311033</td>\n",
       "      <td>2.305778</td>\n",
       "      <td>2.310341</td>\n",
       "      <td>2.309567</td>\n",
       "      <td>2.301748</td>\n",
       "      <td>2.301391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.310978</td>\n",
       "      <td>2.310081</td>\n",
       "      <td>2.315347</td>\n",
       "      <td>2.311033</td>\n",
       "      <td>2.305778</td>\n",
       "      <td>2.310341</td>\n",
       "      <td>2.309567</td>\n",
       "      <td>2.301748</td>\n",
       "      <td>2.301391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glorot</th>\n",
       "      <td>2.26232</td>\n",
       "      <td>0.401366</td>\n",
       "      <td>0.254133</td>\n",
       "      <td>0.211296</td>\n",
       "      <td>0.224231</td>\n",
       "      <td>0.156430</td>\n",
       "      <td>0.166913</td>\n",
       "      <td>0.108587</td>\n",
       "      <td>0.090472</td>\n",
       "      <td>0.088422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7   \\\n",
       "zero    2.30247  2.310978  2.310081  2.315347  2.311033  2.305778  2.310341   \n",
       "normal      NaN  2.310978  2.310081  2.315347  2.311033  2.305778  2.310341   \n",
       "glorot  2.26232  0.401366  0.254133  0.211296  0.224231  0.156430  0.166913   \n",
       "\n",
       "              8         9         10  \n",
       "zero    2.309567  2.301748  2.301391  \n",
       "normal  2.309567  2.301748  2.301391  \n",
       "glorot  0.108587  0.090472  0.088422  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = pd.DataFrame([NN_Mnist_zero.loss_train, NN_Mnist_normal.loss_train, NN_Mnist_glorot.loss_train], \\\n",
    "                       columns =  [1,2,3,4,5,6,7,8,9,10], index = ['zero','normal','glorot'])\n",
    "losses.rename_axis('Initialization').rename_axis('Epoch', axis = 1)\n",
    "print('Average loss by initialization and Epoch')\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = [1,2,3,4,5,6,7,8,9,10]\n",
    "inits = ['zero','normal','glorot']\n",
    "plt.figure(1)\n",
    "plt.grid()\n",
    "plt.ylim(0,3)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average loss')\n",
    "plt.plot([1,2,3,4,5,6,7,8,9,10], NN_Mnist_zero.loss_train, marker = '.')\n",
    "plt.plot([1,2,3,4,5,6,7,8,9,10], NN_Mnist_normal.loss_train, marker = '.')\n",
    "plt.plot([1,2,3,4,5,6,7,8,9,10], NN_Mnist_glorot.loss_train, marker = '.')\n",
    "plt.legend(inits)\n",
    "plt.title('Average loss per epoch for three different weight initializations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Find a set of hyperparameters with validation accuracy over 97%\n",
    "\n",
    "    -512 neurons on each hidden layer\n",
    "    -learning rate schedule is 0.05 for first 7 epochs, 0.01 for the 8th, 0.003 for the last 2. \n",
    "    -ReLu activation function\n",
    "    -Gradient norm cutoff is 1e-6\n",
    "    -Number of epochs where valid_error increases before early stopping occurs is 3 in a row\n",
    "    -Minibatch of size 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missclassification rate after epoch  0.0  :  0.86562\n",
      "Average loss after epoch  0.0  :  2.313954007504873\n",
      "Missclassification rate after epoch  1.0  :  0.10664\n",
      "Average loss after epoch  1.0  :  0.4084078644699645\n",
      "Missclassification rate after epoch  2.0  :  0.06588\n",
      "Average loss after epoch  2.0  :  0.23433235945160294\n",
      "Missclassification rate after epoch  3.0  :  0.05908\n",
      "Average loss after epoch  3.0  :  0.21193338830123462\n",
      "Missclassification rate after epoch  4.0  :  0.05862\n",
      "Average loss after epoch  4.0  :  0.21983803436223964\n",
      "Missclassification rate after epoch  5.0  :  0.04292\n",
      "Average loss after epoch  5.0  :  0.14553325499978245\n",
      "Missclassification rate after epoch  6.0  :  0.04712\n",
      "Average loss after epoch  6.0  :  0.16914304936719124\n",
      "Missclassification rate after epoch  7.0  :  0.03182\n",
      "Average loss after epoch  7.0  :  0.10932588299760193\n",
      "Missclassification rate after epoch  8.0  :  0.02664\n",
      "Average loss after epoch  8.0  :  0.08872028194973433\n",
      "Missclassification rate after epoch  9.0  :  0.02554\n",
      "Average loss after epoch  9.0  :  0.080663122671635\n",
      "Missclassification ratio on training set2.35 %\n",
      "Missclassification ratio on validation set2.9000000000000004 %\n"
     ]
    }
   ],
   "source": [
    "#successful MLP: valid error is _____ missclassification and test error is ______\n",
    "np.random.seed(123)\n",
    "NN_Mnist = NN(784,512,512,10)\n",
    "\n",
    "first = time.time()\n",
    "NN_Mnist.train(X_train, y_train, 10, eta=0.05, max_ite = 50000 , epsilon=1e-12, print_loss=True, \\\n",
    "               X_valid = X_valid, Y_valid= Y_valid, X_test = X_test, Y_test = y_test, \\\n",
    "               lamb = ((0,0.00005),(0,0.00005),(0,0.00005)), print_graphs=True, initialization = 'glorot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy search\n",
    "First we find the optimal stepsize with no regularization. Each tuple contains the r_valid and the hyperparameter that was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:267: RuntimeWarning: overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missclassification ratio on training set88.644 %\n",
      "Missclassification ratio on validation set89.36 %\n",
      "Missclassification ratio on training set1.8739999999999999 %\n",
      "Missclassification ratio on validation set2.5100000000000002 %\n",
      "Missclassification ratio on training set4.2700000000000005 %\n",
      "Missclassification ratio on validation set4.31 %\n",
      "Missclassification ratio on training set7.166 %\n",
      "Missclassification ratio on validation set6.529999999999999 %\n",
      "[(0.8936, 1), (0.0251, 0.1), (0.0431, 0.01), (0.0653, 0.001)]\n"
     ]
    }
   ],
   "source": [
    "stepsizes = [1,1e-1,1e-2,1e-3]\n",
    "r_valids = []\n",
    "for i in range(len(stepsizes)):\n",
    "    NN_Mnist_tune = NN(784,512,512,10)\n",
    "    NN_Mnist_tune.train(X_train, y_train, 10, eta=stepsizes[i], max_ite = 50000 , epsilon=1e-6, print_loss=False, \\\n",
    "                        X_valid = X_valid, Y_valid= Y_valid, X_test = X_test, Y_test = y_test, \\\n",
    "                        print_graphs=True, initialization = 'glorot')\n",
    "\n",
    "    r_valids.append((NN_Mnist_tune.compute_missclass(X_valid,Y_valid),stepsizes[i]))\n",
    "\n",
    "print(r_valids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best stepsize is 0.1, with a validation accuracy of over 97%. We could stop here considering the assignment's goal, but we proceed for pedagogical purposes. We test for values around this within the same order of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missclassification ratio on training set88.644 %\n",
      "Missclassification ratio on validation set89.36 %\n",
      "Missclassification ratio on training set1.8780000000000001 %\n",
      "Missclassification ratio on validation set2.6100000000000003 %\n",
      "Missclassification ratio on training set4.314 %\n",
      "Missclassification ratio on validation set4.35 %\n",
      "[(0.8936, 0.5), (0.0261, 0.1), (0.0435, 0.05)]\n"
     ]
    }
   ],
   "source": [
    "stepsizes2 = [0.5,0.1,0.05]\n",
    "r_valids_2 = []\n",
    "for i in range(len(stepsizes2)):    \n",
    "    NN_Mnist_tune = NN(784,512,512,10)\n",
    "    NN_Mnist_tune.train(X_train, y_train, 10, eta=stepsizes[i], max_ite = 50000 , epsilon=1e-6, print_loss=False,\\\n",
    "                        X_valid = X_valid, Y_valid= Y_valid, X_test = X_test, Y_test = y_test, \\\n",
    "                        print_graphs=True, initialization = 'glorot')\n",
    "\n",
    "    r_valids_2.append((NN_Mnist_tune.compute_missclass(X_valid,Y_valid),stepsizes2[i]))\n",
    "\n",
    "print(r_valids_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best stepsize is 0.1. Add decreasing regime of stepsizes for later iterations(hardcoded into class).\n",
    "\n",
    "Find best lambda for l2 regularization. l1 regularization is coded but not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit to avoid overfit\n",
      "Missclassification ratio on training set78.86800000000001 %\n",
      "Missclassification ratio on validation set77.63 %\n",
      "Missclassification ratio on training set5.04 %\n",
      "Missclassification ratio on validation set4.33 %\n",
      "Missclassification ratio on training set2.2159999999999997 %\n",
      "Missclassification ratio on validation set2.82 %\n",
      "[(0.7763, ((0, 0.1), (0, 0.1), (0, 0.1))), (0.0433, ((0, 0.001), (0, 0.001), (0, 0.001))), (0.0282, ((0, 0.0001), (0, 0.0001), (0, 0.0001)))]\n"
     ]
    }
   ],
   "source": [
    "lambdas = [((0,0.1),(0,0.1),(0,0.1)),((0,0.001),(0,0.001),(0,0.001)),((0,0.0001),(0,0.0001),(0,0.0001))]\n",
    "r_valids_3 = []\n",
    "for i in range(len(lambdas)):\n",
    "    NN_Mnist_tune = NN(784,512,512,10)\n",
    "    NN_Mnist_tune.train(X_train, y_train, 10, eta=0.1, max_ite = 50000 , epsilon=1e-6, print_loss=False, \\\n",
    "                        X_valid = X_valid, Y_valid= Y_valid, X_test = X_test, Y_test = y_test, \\\n",
    "                        lamb = lambdas[i], print_graphs=True, initialization = 'glorot')\n",
    "\n",
    "    r_valids_3.append((NN_Mnist_tune.compute_missclass(X_valid,Y_valid),lambdas[i]))\n",
    "\n",
    "print(r_valids_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best lambda is 0.0001. So far, best value of lambda is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Gradients using Finite Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 different values of N = 1/epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N = 5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.037353</td>\n",
       "      <td>-0.015902</td>\n",
       "      <td>-0.017209</td>\n",
       "      <td>-0.006171</td>\n",
       "      <td>-0.016126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.029403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.021234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N = 50</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.046870</td>\n",
       "      <td>-0.015902</td>\n",
       "      <td>-0.017209</td>\n",
       "      <td>-0.006171</td>\n",
       "      <td>-0.016126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.030808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.021234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N = 500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.046870</td>\n",
       "      <td>-0.015902</td>\n",
       "      <td>-0.017209</td>\n",
       "      <td>-0.006171</td>\n",
       "      <td>-0.016126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.030808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.021234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N = 5000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.046870</td>\n",
       "      <td>-0.015902</td>\n",
       "      <td>-0.017209</td>\n",
       "      <td>-0.006171</td>\n",
       "      <td>-0.016126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.030808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.021234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N = 50000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.046870</td>\n",
       "      <td>-0.015902</td>\n",
       "      <td>-0.017209</td>\n",
       "      <td>-0.006171</td>\n",
       "      <td>-0.016126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.030808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.021234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5    6  \\\n",
       "N = 5      0.0 -0.037353 -0.015902 -0.017209 -0.006171 -0.016126  0.0   \n",
       "N = 50     0.0 -0.046870 -0.015902 -0.017209 -0.006171 -0.016126  0.0   \n",
       "N = 500    0.0 -0.046870 -0.015902 -0.017209 -0.006171 -0.016126  0.0   \n",
       "N = 5000   0.0 -0.046870 -0.015902 -0.017209 -0.006171 -0.016126  0.0   \n",
       "N = 50000  0.0 -0.046870 -0.015902 -0.017209 -0.006171 -0.016126  0.0   \n",
       "\n",
       "                  7    8         9  \n",
       "N = 5     -0.029403  0.0 -0.021234  \n",
       "N = 50    -0.030808  0.0 -0.021234  \n",
       "N = 500   -0.030808  0.0 -0.021234  \n",
       "N = 5000  -0.030808  0.0 -0.021234  \n",
       "N = 50000 -0.030808  0.0 -0.021234  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(111)\n",
    "\n",
    "NN_Mnist = NN(784,512,512,10)\n",
    "NN_Mnist.initialize_weights(initialization = 'glorot')\n",
    "NN_Mnist.forward(X_train[150].reshape(-1,1).T)\n",
    "NN_Mnist.backward(X_train[150].reshape(-1,1).T, y_train[150].reshape(-1,1).T)\n",
    "\n",
    "finite_differences = []\n",
    "for i in range(5):\n",
    "    finite_differences.append(NN_Mnist.finite_differences(X_train[150],y_train[150], epsilon = 1/((5)*10**(i))))\n",
    "finite_differences = np.array(finite_differences)\n",
    "\n",
    "import pandas as pd\n",
    "row_labels = ['N = 5','N = 50','N = 500','N = 5000','N = 50000']\n",
    "column_labels = []\n",
    "pd.DataFrame(finite_differences, index = row_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot max differences as a function of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_differences = np.max(np.abs(finite_differences - NN_Mnist.dW2[0][0:10]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"> Plot of $ \\left(\\max_{1\\leq i \\leq p} | \\nabla_i^N - \\frac{\\partial L}{\\partial \\theta_i}| \\right)$ as a function of N <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29213c89518>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAD8CAYAAABO3GKQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4VdW9//H3NzOBQCCEMAUCYTIgKERkEAgWFanVVuvc1qq3yK22trZWbe+9tr23tWrVOlCVX6XWtldR622tpTggUUFQBmWeEoYQEEIkBAIkIcn6/ZEDjZiEk+GcvU/yeT3PeZKzc/Y+nyxCvll7r72WOecQEREJlSivA4iISNumQiMiIiGlQiMiIiGlQiMiIiGlQiMiIiGlQiMiIiGlQiMiIiGlQiMiIiGlQiMiIiEV43WAcOjevbvLyMho1r5HjhyhY8eOrRuoDVN7NZ3arGnUXk3TkvZauXJlsXMutaUZ2kWhycjIYMWKFc3aNzc3l5ycnNYN1IapvZpObdY0aq+maUl7mdnO1sigU2ciIhJSKjQiIhJSKjQiIhJSKjQiIhJS7WIwQHOsKihhbu5m1uQXM7JgGTflDGV0v65exxIRiTgqNPV4eMEGnn5rHfsWv0jF3jzW9hrEm2uu4pZpI7hjepbX8UREIooKzSlWFZTw9FvryHvyFmqOHgSgfOdqDq1ZyFPMISerl3o2IiJNoGs0p5ibu5l9i188WWROqDl6kP1L5jE3d7NHyUREIpMKzSm27TtIxd68er9Wvjef7UWlYU4kIhLZVGhOMTAtmYReg+r9WkLPTAamdQlzIhGRyKZCc4qbcobSY+JVRCUmf2Z7VGIyqROv5sYpQz1KJiISmTQY4BSj+3XllmkjeIo57F8yj/K9+cSlZdLl3K/yzSnDNBBARKSJVGjqccf0LHKyejF3ZDpr8neR0acn7207hIuK9jqaiEjE0amzBozu15UnvjGOn09N5bmZk7gyuy9/eH8nuw4c9TqaiEhEUaEJ0vcvGEJUFPz6DQ1vFhFpChWaIPXq0oGbzxvA3z7ew9pCDXEWEQmWCk0T3DIlk+G9O3PgaKXXUUREIoYGAzRB54RYXvvOeZiZ11FERCKGejRNZGZUVFUzb3kB1TXO6zgiIr6nQtMM72zez11/WUvu5iKvo4iI+J5OnTXDBVlpvDRrPOdkdPM6ioiI76lH0wxmdrLIHKus9jiNiIi/qdC0wCurCjnv/rcpLqvwOoqIiG+p0LTAyL7JHDx2nMcXbvU6ioiIb0VkoTGzM8zsKTN72cz+3ascg3p04ppz0vnzBwVsLz7iVQwREV8Le6Exs7lmVmRm607ZPt3MNptZnpnd3dgxnHMbnXOzgKuA7FDmPZ3vTRtCXEwUDyzY5GUMERHf8qJH8ywwve4GM4sGZgMXA1nAtWaWZWZnmtlrpzx6BPa5FFgMLAxv/M9KTYrnlsmZ/HPdXlbuLPEyioiIL4W90Djn3gUOnLJ5LJDnnNvmnKsEXgAuc86tdc5dcsqjKHCcV51zE4Drw/sdfN6/TRpAalI8983fiHO6iVNEpC6/3EfTB9hV53khcG5DLzazHOByIB6Y38BrZgIzAdLS0sjNzW1WsLKysqD2/WI/x7PrS3j4xYWMSfNLs4ZfsO0l/6I2axq1V9P4ob388huxvsnDGuwaOOdygdzGDuicmwPMAcjOznY5OTnNCpabm0sw+55XXcPiR9/jH7sc3/3qZGKjI3KcRYsF217yL2qzplF7NY0f2ssvvw0LgfQ6z/sCezzK0iwx0VHcPX0Yw3olUVZe5XUcERHf8EuPZjkw2MwGALuBa4DrvI3UdNOy0piWleZ1DBERX/FiePPzwFJgqJkVmtnNzrkq4DbgdWAj8KJzbn24s7WWvKLDvLo6ojpkIiIhE/YejXPu2ga2z6eBC/uR5tGFeXyw7VMuGp5GfEy013FERDzll1Nnbcp/fPEMYqJMRUZEBBWakEjrnACAc47DFVV0Toj1OJGIiHf8MuqszXHO8Y25H3LHvI+9jiIi4ikVmhAxMyZkduetjUUs2/ap13FERDyjQhNCN07MoFeXBH45fyM1NZqaRkTaJxWaEEqIjeYHFw5lTWEp/1j7iddxREQ8oUITYl85uw/DeibxwOubqKjSss8i0v6o0IRYdJTx4xlnsOvAMf68rMDrOCIiYadCEwaTh6QyaXB3Hn97K6XHjnsdR0QkrFRowuSu6cM4eOw4T+bmex1FRCSsVGjCZESfLtw0cQD9UxK9jiIiElaaGSCM/vOSLK8jiIiEnXo0YVZVXcPzHxawYc8hr6OIiISFCk2YHams5v4Fm/i/jwq9jiIiEhY6dRZmXTrE8uqt55HerYPXUUREwkI9Gg/0S0nEzNh/uEJT04hIm6dC45Et+w4z5cFF/PXj3V5HEREJKRUajwxK7URmaid+/fpmyo9rahoRabtUaDwSFWXcM2MYe0rLefb9HV7HEREJGRUaD03I7M75w3owe1EeJUcqvY4jIhISKjQeu2v6MI5UVPHEojyvo4iIhIQKjceG9kziyjHpPLd0BwWfHvU6johIq1Oh8YE7LhxCdJTx4BubvY4iItLqVGh8IK1zAt+aNJC/r97D6l0HvY4jItKqNDOAT8ycPJCdnx6lY7z+SUSkbdFvNZ9ISojlsWvP9jqGiEir06kzn9l14Ci/nL+Rquoar6OIiLQKFRqfWb+nlOeW7mDDJ1pGQETaBp0685mLhvfk3R9NpUdSgtdRRERaRUT2aMwsx8zeM7OnzCzH6zytyczokZSAc44dxUe8jiMi0mJhLzRmNtfMisxs3Snbp5vZZjPLM7O7T3MYB5QBCUCbXEHskbe28sXH3mP/4Qqvo4iItIgXPZpngel1N5hZNDAbuBjIAq41sywzO9PMXjvl0QN4zzl3MXAX8LMw5w+LL5/Vm/KqGh5duMXrKCIiLRL2QuOcexc4cMrmsUCec26bc64SeAG4zDm31jl3ySmPIufciSFZJUB8GOOHzcDUTlw3th/Pf7iL/P1lXscREWk2vwwG6APsqvO8EDi3oReb2eXARUAy8EQDr5kJzARIS0sjNze3WcHKysqavW9LndPB8ZI5fvSnxXzn7MgYHOBle0UqtVnTqL2axg/t5ZdCY/Vsa3CNY+fcK8ArjR3QOTcHmAOQnZ3tcnJymhUsNzeX5u7bGnbGbOWhN7fQKWMk2RndPMsRLK/bKxKpzZpG7dU0fmgvv4w6KwTS6zzvC+zxKIuv3DxpAD2S4vnl/I0412DtFRHxraALjZl1DGGO5cBgMxtgZnHANcCrIXy/iJEYF8MPLhzCqoKDLFi31+s4IiJNdtpCY2YTzGwDsDHwfJSZ/ba5b2hmzwNLgaFmVmhmNzvnqoDbgNcD7/Oic259c9+jrblidF+GpHXi/gWbOK6paUQkwgRzjeYRai+8vwrgnFttZpOb+4bOuWsb2D4fmN/c47ZlMdFR/McXs9j4ySFqdPpMRCJMUIMBnHO7zD5zvb46NHGkIZOHpDJ5SKrXMUREmiyYazS7zGwC4Mwszsx+SOA0moTf3z7eze+XbPc6hohI0IIpNLOAW6m916UQOCvwXDywcGMR89d+Qk2NTqGJSGQ47akz51wxcH0YskgQfvGVEXSMiyEqqr5bj0RE/CeYUWd/MLPkOs+7mtnc0MaShiQlxBIVZRw8Wsnug8e8jiMiclrBnDob6Zw7eOKJc64E0JrDHqqucXzpicX851/Xnf7FIiIeC6bQRJlZ1xNPzKwb/pm6pl2KjjK+dm5/3t5UxPv5xV7HERFpVDCF5iHgfTP7bzP7b+B94IHQxpLTuWFCBn2SO3Df/E0aGCAivnbaQuOcew74KrAPKAIud879MdTBpHEJsdH84MIhrN1dyt/XaFo4EfGvYOc620TtbMl/A8rMrF/oIkmwvnxWH87o1ZkHX99MRZXuoRURfwpm1Nl3qO3NvAm8Bvwj8FE8FhVl/HjGMApLjvHHpTu9jiMiUq9gLurfDgx1zn0a6jDSdJMG105N8/jbeVw5Jp0uibFeRxIR+YygpqABSkMdRJrv7unDOFR+nN/m5nkdRUTkc4Lp0WwDcs3sH0DFiY3OuYdDlkqaJKt3Z+65eBhjB6R4HUVE5HOCKTQFgUdc4CE+NHNyptcRRETqFcxcZz+D2hU2nXNHQh9JmqvkSCX3L9jE18b1Z0SfLl7HEREBght1Nr41V9iU0ImONhZtLmLtbl1SExH/CObU2W9oxRU2JXQ6J8Tyzp1TSYiN9jqKiMhJQd2w6Zzbdcom3R3oUyeKzIodB6jW1DQi4gNaYbMNej+/mK8+tZRXVhV6HUVERCtstkXjB6Ywqm8XHnpjC+XH1fkUEW81WmjMLBr4unPueudcmnOuh3Pua5olwN/MjHtmnMHeQ+XMXbLd6zgi0s41Wmicc9XAZWHKIq1o3MAUpp3RgycX5XPgSKXXcUSkHQvm1NkSM3vCzCaZ2egTj5Ankxa7a/owjlRW8djCrV5HEZF2LJjhzRMCH39eZ5sDzm/9ONKaBqclcfU5/fjTsp18c0IGGd07eh1JRNqhYBY+m1rPQ0UmQnx/2mBio6N48I3NXkcRkXYqmJkB0szsGTP7Z+B5lpndHPpo0hp6dE7gW5MH8o81n/BRQYnXcUSkHQrm1NmzwO+BnwSebwHmAc+EKJO0spmTBxIfE8XgtCSvo4hIOxTMYIDuzrkXgRoA51wVmhkgonSKj+HWqYPoFB/M3xUiIq0rmEJzxMxSqB0AgJmNQwuhRaTFW4v5tz+soKq6xusoItKOBPMn7h3UTqiZaWZLgFTgqyFNdRpmNgm4ntr8Wc65CafZRYAjlVVsLy7jk9Jy0rsleh1HRNqJBguNmV3pnHsJKAGmAEMBAzY754439w3NbC5wCVDknBtRZ/t04FEgGvidc+5XDR3DOfce8J6ZfRlY3tws7c2FWWl8YVgPYqKDmktVRKRVNPYb557Ax78456qcc+udc+taUmQCngWm190QmOpmNnAxkAVcGxjddqaZvXbKo0edXa8Dnm9hnnbDzIiJjqKsoor384u9jiMi7URjp84OmNkiYICZvXrqF51zlzbnDZ1z75pZximbxwJ5zrltAGb2AnCZc+4+ans/n2Nm/YBS59yh5uRoz37+9/W8tuYTcu/MoUdSgtdxRKSNa6zQzABGA38EHgpxjj5A3TVvCoFzT7PPzdQOu66Xmc0EZgKkpaWRm5vbrGBlZWXN3tevxnSo4S/Hq7nruXe4YXh8qx67LbZXqKnNmkbt1TR+aK/GCs0zzrmvm9n/c869E+IcVs+2Rlftcs7de5qvzwHmAGRnZ7ucnJxmBcvNzaW5+/rZxur1/HHZTn5yZTaDenRqteO21fYKJbVZ06i9msYP7dXYNZoxZtYfuN7MuppZt7qPVs5RCKTXed4X2NPK7yF1fOf8QXSIjeb+BZu8jiIibVxjheYpYAEwDFh5ymNFK+dYDgw2swFmFgdcQ+2QagmRlE7x/HtOJm9u2MeH2w94HUdE2rAGC41z7jHn3BnAXOfcQOfcgDqPgc19QzN7HlgKDDWzQjO7OTDbwG3A69QuE/2ic259c99DgnPTxAH07JzAL+dvxLlGz1SKiDRbY/fRdA6M6PpJfafKnHPN+jPYOXdtA9vnA/Obc0xpng5x0dxx4RB+9PIa/rluLzPO7OV1JBFpgxo7dfa/gY8nTpWF8tSZeOSK0X0ZmpbEAws2aWoaEQmJBns0zrlLAh8HhC+OhFt0lPE/XxlBVbXTjAEiEhKNnTprdLlm59yq1o8jXjgno7UHEYqI/Etj99GcuEkzAcgGVlN7v8tI4APgvNBGk3ByzvHfr22kU0IMd1wwxOs4ItKGNDbqbKpzbiqwExjtnMt2zo0BzgbywhVQwsPMKD12nMPlLZ3KTkTks4JZJmCYc27tiSfOuXVmdlYIM4lHfn3lSMzqm6RBRKT5gik0G83sd8CfqJ0W5mvU3usibcyJIrOqoITOCbGtOjWNiLRfwQwzuhFYD9wOfA/YENgmbdCxympu/P1y/ucfG7yOIiJtxGkLjXOu3Dn3iHPuK4HHI8658nCEk/DrEBfNbVMHkbt5P0vytGaNiLScbpyQz/n6+P70Se7AL+dvpKZGU9OISMuo0MjnJMRGc+dFQ1m/5xCvrtYk2iLSMqctNGb2uSUYzax7aOKIX1w6qjcj+nTmwdc3U3682us4IhLBgunRLDezcSeemNkVwPuhiyR+EBVl/PjiM9h98BjPLd3hdRwRiWDBDG++DphrZrlAbyAFOD+UocQfJgzqTs7QVJ54O4+rstNJTozzOpKIRKBgRp2tBX4BzAKmArc55wpDHUz84e6Lh3G4ooqn393mdRQRiVCn7dGY2TNAJrVznA0B/m5mTzjnZoc6nHhvWM/OPH7t2UwalOp1FBGJUMFco1kHTHXObXfOvQ6MAxqd2VnalktG9qZLYqxW4RSRZgnm1Nkjrs5vGOdcqXPu5tDGEr/J31/GJY8vZm1hqddRRCTCBHPqbDBwH5BF7ZIBADjnBoYwl/hMalI8MdFRmt1ZRJosmFFnvwfuBR6hdjDAjdSuSyPtSOeEWP5260SvY4hIBArmGk0H59xCwJxzO51zP0XDm9ut8uPV/GnZTqo1NY2IBCmYHk25mUUBW83sNmA30CO0scSvcjfv5z/+uo646CiuOifd6zgiEgGC6dF8D0gEvguMAb4O3BDKUOJfFw1P46z0ZB56czPHKjU1jYicXjCjzpY758qcc4XOuRudc5c755aFI5z4j5nx4xlnsO9QBXOXbPc6johEgGAm1cw2s/8zs1VmtubEIxzhxJ/GDujGBVlpPJmbT3FZhddxRMTngjl19mdqR55dAXypzkPasbumD+PY8WoeX7jV6ygi4nPBFJr9zrlXAzMD7DzxCHky8bVBPTpxzTnp/PmDArYXH/E6joj4WDCF5l4z+52ZXWtml594hDyZ+N7t0wYTFxPFAws2eR1FRHwsmOHNNwLDgFigJrDNAa+EKpREhh5JCcycPJDfvLWVjZ8c4oxenb2OJCI+FEyhGeWcOzPkSSQifWvSQEalJzOsZ5LXUUTEp4IpNMvMLMs5tyHkaYJgZlnAT4FPgYXOuZe9TdS+dYyPYerQHqwqKOGZ3E2szS9mZMEybsoZyuh+Xb2OJyI+EEyhOQ+4wcy2AxXUznPmnHMjm/pmZjYXuAQocs6NqLN9OvAoEA38zjn3q0YOczHwuHPuPTN7FVCh8djDCzYw+421FL//EpV781jbaxBvrrmKW6aN4I7pWV7HExGPBVNoprfi+z0LPAE8d2KDmUUDs4ELgEJgeaCARFM7a3RdNwF/pHaAwqXUListHlpVUMLTb61jx9OzqDl6EIDynas5tGYhTzGHnKxe6tmItHOnLTStOZTZOfeumWWcsnkskOec2wZgZi8Alznn7qO291OfWwMFqsEBCWY2E5gJkJaWRm5ubrMyl5WVNXvf9uC3K0rZt3jeySJzQs3RgxQtnsevesbx7ewuHqWLDPoZaxq1V9P4ob2C6dGEWh9gV53nhcC5Db04UKh+DHQEHmzodc65OcAcgOzsbJeTk9OscLm5uTR33/bggeULqNibX+/XKvblc4QEtd9p6GesadReTeOH9grmPppQq29tmwbnoHfO7XDOzXTOXe+cWxzCXBKEgWnJJPQaVO/X4tIyOVTp+KT0WJhTiYif+KHQFAJ155vvC+zxKIs00U05Q+kx8SqiEpM/sz0qMZlu469kT1k1Ux7M5ed/36B50UTaKT+cOlsODDazAdSudXMNcJ23kSRYo/t15ZZpI3iKOexfMo/yvfkk9MwkdeLVzJo2nCvHZvDYwq08+/52XlhewI0TM5g5KZMuibFeRxeRMAlroTGz54EcoLuZFQL3OueeCSyo9jq1I83mOufWhzOXtMwd07PIyerF3JHprMnfxcjM9M/cR/PglaOYlZPJI29uYfaifLYXH+G314/xOLWIhEtYC41z7toGts8H5oczi7Su0f26Mvob48jNLScnZ9znvp6Z2oknrhvNt3MOERdTe1lu14GjvLFhH9ef24+E2OhwRxaRMPHDNRppR7J6d2ZQj9rpav6x9hMeWLCJ0mPHPU4lIqGkQiOemTUlkze/P4W0zgk45/jhS6v560e7qa5pcNChiEQgFRrxVL+URABKjx1n3e5SvjfvYy5+9F0WrNuLcyo4Im2BCo34QnJiHPO/O4nHrz2bqmrHrD+t5Muzl/Dulv0qOCIRToVGfCMqyvjSqN688f3JPHDFSIrLKvnG3A+5es4ylu844HU8EWkmFRrxnZjoKK46J523fziFn106nO3FR7jyqaXc/sJHXkcTkWbwww2bIvWKj4nmhgkZXJWdzh+W7qBDYAh0TY1jx6dHGJjayduAIhIU9WjE9zrERTNrSiY3TMgA4LW1nzDt4XdYubPE22AiEhQVGok45w3qzo+mD+Ps9Nr51d7dsp+9peUepxKRhujUmUScbh3jmDUlE4DKqhrueHE1h8uP843x/fn3nEF06xjncUIRqUs9GolocTFR/N+3J3DJyN48s3g7k+5/m4ff2Myhcs02IOIXKjQS8dK7JfLQVaN44/uTmTI0lcfezmPS/Yt4Mjefo5VVXscTafdUaKTNGNQjid9eP4bXvnMeo/slc/+CTUx+IJdnl2ynRtPaiHhGhUbanBF9uvD7G8fy8qzxDOrRkfnr9mKBdVw1y4BI+KnQSJuVndGN5781jmduyMbM2HeonBmPLWblTs0yIBJOKjTSppkZSQm1q3nuP1xBdBSkdkoA4FD5cfVwRMJAw5ul3RjRpwt/v+08LHAe7fbnP6Lk6HHuvGgoEwd19zidSNulHo20KyeKjHOOi4b3pOhQOdf/7gOunbNMMw2IhIgKjbRLZsY1Y/vx9g9zuPdLWWwtOswVT77PTc8uZ/2eUq/jibQpKjTSriXERnPjxAG8+6Op3HnRUFbsOMAXH1vMrf+7ivz9ZV7HE2kTVGhEgMS4GG6dOoj37jqf75w/iEWbivjy7CW64VOkFWgwgEgdXTrE8oMLh/LNCRms2V1KYlwMzjmefncbXzm7D2mdE7yOKBJx1KMRqUdKp3imDu0BwOZ9h/n165tZtKnI41QikUk9GpHTGNazM2//IIdeybW9mXnLC9hzsJx/mzTg5D06ItIw9WhEgtAvJZHY6Nr/LmsKS3l04VYmPbCIp9/J51hltcfpRPxNhUakiX7xlTN59baJjOybzH3/3MSUBxfx3NIdVFbVeB1NxJdUaESaYWTfZJ67aSwv3jKejJSO/Nff1nP+Q7m8tGIXVdUqOCJ1qdCItMDYAd2Yd8s4/nDTWLomxnHny2u48Dfvsu+QlpYWOUGDAURayMyYMiSVyYO78/r6fSxY9wk9kuIB2FF8hP4piSenvhFpj3zfozGzgWb2jJm93Ng2Ea+ZGdNH9OQ315yNmVFypJJLHl/Mr9/Y7HU0EU+FtNCY2VwzKzKzdadsn25mm80sz8zubuwYzrltzrmbT7dNxG86JcTw4xln8JWz+wC1vZuPCv41ceeqghJue24Z/7WomNueW8aqAk3qKW1TqE+dPQs8ATx3YoOZRQOzgQuAQmC5mb0KRAP3nbL/Tc453SUnESk2Oorrzu138vnsRXm8tLKQaWekkdoxileW5bFv8YtU7M1jba9BvLnmKm6ZNoI7pmd5mFqk9YW00Djn3jWzjFM2jwXynHPbAMzsBeAy59x9wCWhzCPipXsvHU6/bok8+U4+h0tL2PO7b1Nz9CAA5TtXc2jNQp5iDjlZvRjdr6vHaSXSrSooYW7uZtbkFzOyYBk35Qz17OfKi8EAfYBddZ4XAuc29GIzSwF+AZxtZvc45+6rb1s9+80EZgKkpaWRm5vbrLBlZWXN3rc9Uns17sxoGNapgoVvvHyyyJxQc/QgRYvn8auecXw7u4tHCf1PP2On98qmo8zfUkrx+y9TsTefNT0HseDjK5gxpAuXD0sMex4vCk19w28aXE/XOfcpMOt02+rZbw4wByA7O9vl5OQ0OShAbm4uzd23PVJ7nd4DyxdQuTe/3q9V7MvnCAmMPGcCqwsPMm5ACh3iosOc0N/0M9a4VQUlvP7am2x/+rM95sNrF7Lg3+fwzRnjw96z8WLUWSGQXud5X2CPBzlEPDEwLZmEXoPq/VpCz0wGpnXh3S37ufH3y0+uibPxk0O8s2U/Ryq0bIE07v7XVrN38Yv19pj3L5nH3Nzwj4L0okezHBhsZgOA3cA1wHUe5BDxxE05Q3lzzVUcWrPwM78MohKTSZ14NTdOGcoZPTvzp5vP5YxenQH48wc7+dOyAmKijJF9uzA+M4XxA7szpn9X9XjaqeoaR3SUsWnvIe58aQ0/vXQ4Y/p3ZfenZVTuzat3n/K9+WwvCv8KsiEtNGb2PJADdDezQuBe59wzZnYb8Dq1I83mOufWhzKHiJ+M7teVW6aN4CnmsH/JPMr35pPQM5PUiVcza9rwk6c1zhvc/eQ+91x8Bhdm9WTptk9Ztu1TnnpnG7MX5RMbbZyVnsy4gSmMz0xhQmb3ht5WIlhVdQ0bPznMip0HWLmzhFU7S7hhQga3TMmke6d4OsZHU11TewXirP7dWd9rEOU7V3/uOCd6zOEW6lFn1zawfT4wP5TvLeJnd0zPIierF3NHprMmfxcjM9MbHRXUMT6GyUNSmTwkFYCyiipW7DgQKDwHmL0oj7c2FvHP2ycB8MqqQjJTOzEqPTls35O0ntKjx1m1q4SVO0pYubOEj3cd5Njx2lnCe3dJYExGNwandQKge6d4Xpg5/uS+wfSYw01T0Ih4ZHS/roz+xjhyc8vJyRnXpH07xceQM7QHOYHF2Q6XHz85v1p1jePeV9fz5bP6MCo9meoax5O5eZyT0Y2z+iUTH6NTbX6z89MjFJdVMKZ/N5xz5Px6ESVHjxMdZWT16szV56Qzpn9XxvTvSu/kDo0eK9geczip0Ii0AUkJsScXYYuOMpbcfT7lgXVytu0v46E3t+AcxMdEMaZ/V8YPTGFcZgqj+iYTF+P7majalPLj1azdXcrWfWUnb+j96avrKSw5xpt3TMHM+Omlw0k/iPMVAAAILUlEQVRNiues9GQS45r+a7qpPeZQU6ERaYM6J8TSOVB4Bqcl8fF/XsiHOw6wNL/2Gs/Db23BvQkJsVFk9+/G+MwULh/dh15dGv9rWZqu6HA5q3aWsGJHCSsLSli3u5Tj1Q4z+OLIXnTpEMsPLxpKXPS/Cv5lZ/Vp8fu2pMfc2lRoRNqBLomxXJCVxgVZaQAcPFrJB9v/VXgefH0zkwen0qtLB5bvOMDyHQe4YXwGHeP1K6I5Vuw4wJ8/KGDFzgPsOnAMgLiYKEb17cJN5w1gTL/a02BdOtT+MTC8d9u+QVc/RSLtUHJiHBcN78lFw3sCcOBI5clfekvzP+Xpd/L51qSBALzwYQEHjx1n3MAURvTuTEy0TrWdKq/oMD/7+wbumj6MEX26UHS4gve2FpPdvyvfGJfBmIyuDO/dud1eH1OhERG6dYw7+fl3vzCYGydmEBsoKO9s2c8/1+0FagchnJPRlfGZKYwbmMLw3l2Ijmofa+045ygsOcbKnbUjwVbsLOHq7L58c+IAkhJi2X+4gtJjxwG4aHhPLh7RU+sQBajQiMjnnBhYAPDk18aw/3AFywL38Czd9imLNu+vfV18DGMHdGPGmb24Ykxfr+KGRGVVDev3lNbet1JQe42l6HAFUFtwz+6XTGpSAgBpnRNY8L3JJ/dtL8U3WCo0InJaqUnxfGlUb740qjcA+w6Vnyw8y7YdYFVBCVeM6UtNjeP2eR9zVXZfJg1O9Th10xw4UknR4XKG9eyMc44Jv3qb4rLawtK3awcmZKYEhhh3Y2jPJBWTJlChEZEmS+ucwGVn9Tk5Oup4dQ0ARYcrWFN4kPOH1RaZvKIyHliw6eTMBUPTkojywS/omhrHtuIytuwrY8aZvQC4/YWPKC6r5J+3T8LMuH3aYFI6xjGmf1fSOid4nDiyqdCISIuduJ7Ts0sC79w5Fedqp0PZd6icjXsP8caGfQB0TYzl3AEpJ6/xDEnrFJbrGMcqq/l410FWFZScvMZSeuw4UQar772QpIRYvvuFwZ/Z5+vj+oc8V3uhQiMire5E8Zg4qDvv/eh8CkuOsmzbgdprPPmfsmB97eCClI5xnDuwGz+/bATdO8U3esymLuS1trCUVz4qZOXOEjbsOURVYC6wQT06MX14z9rTYBld6RQYwn1ORrfW+NalHio0IhJyfbsm8tUxiXw1MGBg14GjJycIXb3r4MmbS2cvyiN/fxkPXTnqMz2dhxds4Om31jW69PW2/WX85q2t3Hb+IIakJbGtuIznPyxgVN9kZk4eSHZGV85O70rXOiPsJDxUaEQk7NK7JZLeLZGrstM/s72yqiZw13xtkZn53AqOVlazdMMOdjw963NLXz9W9STVRHHn9GHERkfx4fYD7Dl4jCFpSUwf0ZMZZ/Y6eVpPvKNCIyK+8f0Lhpz83DlHp/gYFm0opHhJ/Qt5lSx7mcUDenDn9GH07dqBpfecf7JItdebI/1IpV5EfMnMePjqsxickkDlvvqXvq7cl0/V8eMnX68bJP1JhUZEfC2Ypa/F31RoRMTXbsoZSo+JVxGV+NlF3LxcyEuaRtdoRMTX/LiQlzSNCo2I+J7fFvKSplGhEZGI4KeFvKRpdI1GRERCSoVGRERCSoVGRERCSoVGRERCyk5M592Wmdl+YGczd+8OFLdinLZO7dV0arOmUXs1TUvaq79zrsUr2LWLQtMSZrbCOZftdY5IofZqOrVZ06i9msYP7aVTZyIiElIqNCIiElIqNKc3x+sAEUbt1XRqs6ZRezWN5+2lazQiIhJS6tGIiEhItatCY2bOzB6q8/yHZvbTVjhurpltNrOPA48eLT2mV8LdRmYWb2bzzCzPzD4ws4yWvleo+amNzOyewPbNZnZRSzO0lkhoIzObHtiWZ2Z3tzRbM76XiG0jMxsQOMbWwDHjGsvUrgoNUAFcbmbdQ3Ds651zZwUeRSE4friEu41uBkqcc4OAR4D7Q/C+rc0XbWRmWcA1wHBgOvBbM/PL+sW+bqNAO80GLgaygGsDrw2nSG6j+4FHnHODgZLAsRvU3gpNFbUXxr7vdRAfC3cbXQb8IfD5y8AXzP/r8fqljS4DXnDOVTjntgN5wNgwZTodv7fRWCDPObfNOVcJvBB4bThFZBsF9jk/cAwCx/xyY2/cHpcJmA2sMbMHGnqBmU2ltuKf6qhzbkIDu/3ezKqBvwD/4yJ7lEU426gPsAvAOVdlZqVACv6/89sPbdQHWFZn38LANr/wexvtOmX7uaf/llpdJLZRCnDQOVdVz+vr1e4KjXPukJk9B3wXONbAaxYBZzXhsNc753abWRK1/7BfB55rcViPhLmN6uu9+L5I+6SNfN12Pm+j+s7mhL3tIrSNmvxz195OnZ3wG2rPKXas74tmNrXOhbS6j/fre71zbnfg42Hgf/HP6YuWCFcbFQLpgWPGAF2AA637rYSM1210cntAX2BPy7+tVuXXNvJT20VaGxUDyYFj1N3eMOdcu3kAZXU+fwAoAH7awmPGAN0Dn8dSe95yltffa6S0EXAr8FTg82uAF71ug0hpI2ov3q4G4oEBwDYg2uv2iYQ2ChxrW2BbXOA1w9VGwbUR8BJwTeDzp4BvN5rL6x9ID/9h04CjrfAP2xFYCawB1gOP+uU/eyS0EZAQ+KHNAz4EBnrdBpHURsBPgHxgM3Cx120TSW0EzAC2BL72E7VR8G0EDAwcIy9wzPjGcmlmABERCan2eo1GRETCRIVGRERCSoVGRERCSoVGRERCSoVGRERCSoVGRERCSoVGRERCSoVGRERC6v8DVhBaNXM2w8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('max difference')\n",
    "plt.grid()\n",
    "plt.plot(row_labels, max_differences, '-.', marker = '.', ms = '15', mfc = 'black')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
